{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Eurlex_Label2Glove_Embedding.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMR9BlSIRJUE3ZQ4nOsuSjU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stxupengyu/Milti-Label-Classification-Data-Preprocessing/blob/master/Eurlex_Label2Glove_Embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjuCh5cTZNRu",
        "outputId": "b1c69f79-85f8-4d61-8dc4-652cc0d57c3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#挂载云盘 \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0x1qkkh8ZNPS",
        "outputId": "caef0141-43bb-4d09-ffef-0508bd6abe2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "#进入项目目录\n",
        "print('original document')\n",
        "!ls\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive/colab/XML')\n",
        "print('current document')\n",
        "!ls"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original document\n",
            "eurlex\t\t\t\t    label_coocurrence_freq.csv\n",
            "Eurlex_data_analysis.ipynb\t    label_coocurrence_freq.gsheet\n",
            "Eurlex_Label2Glove_Embedding.ipynb  rcv1.topics.hier.orig.txt\n",
            "EURLex.py\t\t\t    rcv_embedding_mean.npz\n",
            "glove\t\t\t\t    rcv_embedding.npz\n",
            "current document\n",
            "eurlex\t\t\t\t    label_coocurrence_freq.csv\n",
            "Eurlex_data_analysis.ipynb\t    label_coocurrence_freq.gsheet\n",
            "Eurlex_Label2Glove_Embedding.ipynb  rcv1.topics.hier.orig.txt\n",
            "EURLex.py\t\t\t    rcv_embedding_mean.npz\n",
            "glove\t\t\t\t    rcv_embedding.npz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GMndyP3apgn",
        "outputId": "455433d5-089c-425f-f051-bda6262bda01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "#下载glove\n",
        "#!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "#!ls\n",
        "#!unzip glove.6B.zip\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.corpus import stopwords \n",
        "\n",
        "glove_dim=300\n",
        "data_path='eurlex/3714/label_vocab.txt'\n",
        "save_path='eurlex_embedding_mean.npz'\n",
        "\n",
        "def body_remove_sw(sentence,sw):\n",
        "    #去除stopwords 并把句子转换成单词的列表\n",
        "    words= [w for w in sentence.split(' ') if w not in sw]\n",
        "    return words\n",
        "def body_wnl(words,wnl):\n",
        "    #做词形还原\n",
        "    words=[wnl.lemmatize(word) for word in words]\n",
        "    return words\n",
        "def preprocessing(sentence,del_estr,sw,wnl):\n",
        "  #处理每个原始标签\n",
        "    sentence=sentence.lower()#字母小写化\n",
        "    replace = \" \"*len(del_estr)\n",
        "    tran_tab = str.maketrans(del_estr, replace)\n",
        "    sentence = sentence.translate(tran_tab)#完成上述去除标点符号的功能\n",
        "    words=sentence.split(' ')#根据空格把句子分隔成单词\n",
        "    words=body_remove_sw(sentence,sw)#去除停用词\n",
        "    while '' in words:\n",
        "      words.remove('')#去除为空格的空值\n",
        "    words=body_wnl(words,wnl)#词性还原\n",
        "    return words\n",
        "def read_data(data_path):\n",
        "  #读取原始数据，得到处理好的标签列表\n",
        "    iFile = open(data_path, \"r\")\n",
        "    label=[]\n",
        "    wnl = WordNetLemmatizer() \n",
        "    sw=stopwords.words('english')#这两步都是在循环前调用为了加快速度\n",
        "    del_estr = string.punctuation + string.digits+string.whitespace  # 去除ASCII 标点符号，数字\n",
        "    for line in iFile.readlines():\n",
        "        lineVec = line.strip().split(\"\\t\")#根据:将一行分割\n",
        "        label_i=lineVec[-1]\n",
        "        label_i=preprocessing(label_i,del_estr,sw,wnl)\n",
        "        label.append(label_i)\n",
        "    print('有%d个标签'%len(label))\n",
        "    return label \n",
        "label=read_data(data_path)[:]\n",
        "#label#展示一下处理好的label"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "有3714个标签\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSXSQ1SxwEJ4",
        "outputId": "97b85c39-962a-46e2-e91e-9297f1a09d08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "def sum_tag_words(label):\n",
        "  #统计所有标签词\n",
        "  sum_tag=[]\n",
        "  for label_i in label:\n",
        "    for i in label_i:\n",
        "      if i not in sum_tag:\n",
        "        sum_tag.append(i)\n",
        "  return sum_tag\n",
        "def glove_dict(glove_dim):\n",
        "  #构建一个当前标签词在glove的embedding字典\n",
        "  all_tag_words=sum_tag_words(label)\n",
        "  print('使用的是 glove embedding dim=%d'%glove_dim)\n",
        "  #所有的标签词的list，用于从glove中抽取词表示\n",
        "  iFile = open(\"glove/glove.6B.%dd.txt\"%glove_dim, \"r\")#有119万个词\n",
        "  gloveDict = {}#glove word 对应id的字典\n",
        "  count=0\n",
        "  for line in iFile.readlines():\n",
        "      lineVec = line.strip().split(\" \")#将一行转化为一个列表\n",
        "      if lineVec[0] in all_tag_words:\n",
        "        gloveDict[lineVec[0]]=lineVec[1:]\n",
        "  print('%d个tag词有%d有glove embedding'%(len(all_tag_words),len(gloveDict)))\n",
        "  return gloveDict\n",
        "def str2float(str_list):\n",
        "  #将glove的词向量权重从str转化为float np.array\n",
        "  record=[]\n",
        "  for i in str_list:\n",
        "    record.append(float(i))\n",
        "  return np.array(record)\n",
        "def many2one(label_embedding_i):\n",
        "  #将多个词的embediding合并为二维array，并取均值\n",
        "  if label_embedding_i==[]:#考虑到缺失词的情况\n",
        "    return np.zeros(glove_dim)\n",
        "  label_embedding=label_embedding_i[0]\n",
        "  for inst in label_embedding_i[1:]:\n",
        "    label_embedding=np.vstack((label_embedding,inst))\n",
        "  if len(label_embedding.shape)>1:\n",
        "    label_embedding=np.mean(label_embedding,axis=0)#均值操作\n",
        "  return label_embedding\n",
        "def glove_embedding(label):\n",
        "  #将原始标签表示为embedding\n",
        "  label_embedding_list=[]\n",
        "  bad_label_count=0\n",
        "  for label_i in label:\n",
        "    label_embedding_i=[]\n",
        "    for word in label_i:\n",
        "      if word in glovedict.keys():\n",
        "        label_embedding_i.append(str2float(glovedict[word]))\n",
        "      #else:\n",
        "        #print(word)展示找不到embedding的词\n",
        "    label_embedding_i=many2one(label_embedding_i)\n",
        "    if sum(label_embedding_i)==0:#统计一下全是缺失词的标签\n",
        "      bad_label_count+=1\n",
        "    label_embedding_list.append(label_embedding_i)\n",
        "  print('有%d个标签的glove embedding为全0'%bad_label_count)\n",
        "  return label_embedding_list\n",
        "glovedict=glove_dict(glove_dim)\n",
        "label_embedding=glove_embedding(label)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "使用的是 glove embedding dim=300\n",
            "2395个tag词有2357有glove embedding\n",
            "有45个标签的glove embedding为全0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdzQYy5o--3p",
        "outputId": "c9780325-9dbc-4c5e-d443-28f0eb6a5bce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "def save_data(label_embedding):\n",
        "  label_embedding=np.array(label_embedding)\n",
        "  np.savez(save_path,label_embedding=label_embedding,allow_pickle=True)\n",
        "  print('数据集已经保存成功在文件%s中！'%(save_path))\n",
        "def load_data():\n",
        "  data=np.load(save_path,allow_pickle=True)\n",
        "  label_embedding=data['label_embedding']\n",
        "  return label_embedding\n",
        "save_data(label_embedding)\n",
        "label_embedding=load_data()\n",
        "print('输出数据格式为：',label_embedding.shape)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "数据集已经保存成功在文件eurlex_embedding_mean.npz中！\n",
            "输出数据格式为： (3714, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uo0h7UY69qgq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}